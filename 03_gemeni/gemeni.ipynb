{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "194dd133",
   "metadata": {},
   "source": [
    "## Gemeni Api intro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e15d772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It learns from data to find patterns and make smart decisions.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# looks automatically after the key\n",
    "# one of GOOGLE_API_KEY and GEMINI_API_KEY\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Explain how AI works in a few words\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96ddc57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 data engineering jokes, structured in short points:\n",
      "\n",
      "1.  **What's a data engineer's favorite bedtime story?**\n",
      "    The one where the source data is clean, well-structured, and fully documented.\n",
      "\n",
      "2.  **Why did the data pipeline get sent to therapy?**\n",
      "    It had too many dependency issues and kept breaking down in production.\n",
      "\n",
      "3.  **What's the difference between a data lake and a data swamp?**\n",
      "    A data swamp has no metadata, just raw, uncatalogued sadness.\n",
      "\n",
      "4.  **A business analyst asks a data engineer for \"just a quick data pull.\"**\n",
      "    The data engineer knows this is code for \"a 3-week project involving schema changes, API integrations, and existential dread.\"\n",
      "\n",
      "5.  **What's a data engineer's most common excuse for a broken pipeline?**\n",
      "    \"It worked perfectly fine in staging!\"\n"
     ]
    }
   ],
   "source": [
    "def ask_gemini(prompt, model = \"gemini-2.5-flash\"):\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=prompt,\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "response = ask_gemini(\"Give me 5 some data engineering jokes, structure it in short points\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c65cf2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "# knows that GenerateContentResponse is a pydantic model\n",
    "# -> we can work with it in a OOP manner\n",
    "isinstance(response, BaseModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86226959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sdk_http_response', 'candidates', 'create_time', 'model_version', 'prompt_feedback', 'response_id', 'usage_metadata', 'automatic_function_calling_history', 'parsed'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b97f850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemini-2.5-flash'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "080a1f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HttpResponse(\n",
       "  headers=<dict len=11>\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.sdk_http_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee88388b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Candidate(\n",
       "   content=Content(\n",
       "     parts=[\n",
       "       Part(\n",
       "         text=\"\"\"Here are 5 data engineering jokes, structured in short points:\n",
       " \n",
       " 1.  **What's a data engineer's favorite bedtime story?**\n",
       "     The one where the source data is clean, well-structured, and fully documented.\n",
       " \n",
       " 2.  **Why did the data pipeline get sent to therapy?**\n",
       "     It had too many dependency issues and kept breaking down in production.\n",
       " \n",
       " 3.  **What's the difference between a data lake and a data swamp?**\n",
       "     A data swamp has no metadata, just raw, uncatalogued sadness.\n",
       " \n",
       " 4.  **A business analyst asks a data engineer for \"just a quick data pull.\"**\n",
       "     The data engineer knows this is code for \"a 3-week project involving schema changes, API integrations, and existential dread.\"\n",
       " \n",
       " 5.  **What's a data engineer's most common excuse for a broken pipeline?**\n",
       "     \"It worked perfectly fine in staging!\"\"\"\"\n",
       "       ),\n",
       "     ],\n",
       "     role='model'\n",
       "   ),\n",
       "   finish_reason=<FinishReason.STOP: 'STOP'>,\n",
       "   index=0\n",
       " )]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa69b79",
   "metadata": {},
   "source": [
    "### Tokens\n",
    "\n",
    "- basic unit of text for LLMs\n",
    "- can be as short as one character or as long as one word\n",
    "\n",
    "- tokens used for billing\n",
    "\n",
    "Gemeni free tier\n",
    "- Requests per minute (RPM): 10\n",
    "- Tokens per minute (TPM): 250 000\n",
    "- Requests per dagy. (RPD): 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebf008bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=199,\n",
       "  prompt_token_count=15,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=15\n",
       "    ),\n",
       "  ],\n",
       "  thoughts_token_count=1497,\n",
       "  total_token_count=1711\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6535d939",
   "metadata": {},
   "source": [
    "## Thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a65f2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 data engineering jokes, structured in short points:\n",
      "\n",
      "1.  **Why did the data engineer break up with the API?**\n",
      "    *   Too many unhandled exceptions.\n",
      "    *   Their relationship just wasn't schema-compliant.\n",
      "\n",
      "2.  **A data engineer, a data scientist, and a business analyst walk into a bar.**\n",
      "    *   The business analyst asks, \"Where's the bathroom?\"\n",
      "    *   The data scientist asks, \"Is it the men's or women's?\"\n",
      "    *   The data engineer says, \"I'll need to build a pipeline to figure that out, but first, what's your source system?\"\n",
      "\n",
      "3.  **What's a data engineer's favorite type of music?**\n",
      "    *   Stream processing.\n",
      "\n",
      "4.  **How do you know you're talking to a data engineer?**\n",
      "    *   They keep asking, \"But where did the data come from?\"\n",
      "    *   And their response to everything is, \"It depends on the volume and velocity.\"\n",
      "\n",
      "5.  **A data engineer's ideal vacation spot?**\n",
      "    *   Anywhere with robust logging and monitoring.\n",
      "    *   And no unexpected schema changes.\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "prompt =\"Give me 5 some data engineering jokes, structure it in short points\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=prompt,\n",
    "    config=types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "    ),\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3947fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=265,\n",
       "  prompt_token_count=15,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=15\n",
       "    ),\n",
       "  ],\n",
       "  total_token_count=280\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9176cdca",
   "metadata": {},
   "source": [
    "## System instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83849d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOP (Object-Oriented Programming):  Organizes code around \"objects\" containing data (attributes) and functions (methods) that operate on that data. Key principles: encapsulation, inheritance, and polymorphism.\n",
      "\n",
      "Dunder (Double Underscore) Methods: Special methods in Python (e.g., `__init__`, `__str__`) that provide a way to implement operator overloading, customize class behavior, and interact with built-in functions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "You are an expert in Python programming you will always provide idiomatic code,\n",
    "So when you see my code or my question, be very critical, but answer in a SHORT and CONCISE way.\n",
    "Also be constructive to help me iprove\n",
    "        \"\"\"\n",
    "\n",
    "prompt = \"\"\"\"\n",
    "Explain OOP and dunder methods\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=prompt,\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print (response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e82ef10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=92,\n",
       "  candidates_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=92\n",
       "    ),\n",
       "  ],\n",
       "  prompt_token_count=59,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=59\n",
       "    ),\n",
       "  ],\n",
       "  total_token_count=151\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec9a0907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=37,\n",
       "  prompt_token_count=262,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=4\n",
       "    ),\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.IMAGE: 'IMAGE'>,\n",
       "      token_count=258\n",
       "    ),\n",
       "  ],\n",
       "  thoughts_token_count=317,\n",
       "  total_token_count=616\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = response.usage_metadata\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9af54475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 40)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompt.split()), len(system_instruction.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b619aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata.candidates_token_count = 37\n"
     ]
    }
   ],
   "source": [
    "print(f\"{metadata.candidates_token_count = }\") # output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488c65ea",
   "metadata": {},
   "source": [
    "## Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5df04959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gray rabbit twitched its nose, sensing the distant rumble of a lawnmower and immediately darted into the overgrown rose bushes. Safe within the thorny embrace, it nibbled on a fallen petal, the sweet scent masking the fear that still lingered in its twitching whiskers.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "story = \"write a 2 sentence story about a gray rabbit\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=story,\n",
    "    config=types.GenerateContentConfig(temperature=0.0),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68ae7ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gray rabbit twitched his nose, sensing the faint scent of carrots wafting from the nearby farmer's garden. With a daring leap, he hopped the fence, ready for a midnight feast under the silvery moonlight.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "story = \"write a 2 sentence story about a gray rabbit\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=story,\n",
    "    config=types.GenerateContentConfig(temperature=1.0),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "021150c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dusty, the gray rabbit, twitched his nose, sniffing the crisp morning air before venturing out of his burrow in search of a tasty clover patch. Little did he know, a fox with the same craving was already planning its breakfast from a distance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "story = \"write a 2 sentence story about a gray rabbit\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=story,\n",
    "    config=types.GenerateContentConfig(temperature=2.0),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443800e6",
   "metadata": {},
   "source": [
    "## Multimodal input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fed2043d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A fluffy grey rabbit is sitting on a carpet, wearing a small, plush Swedish graduation cap (\"studentm√∂ssa\") on its head and a blue and yellow ribbon draped over its back.\n"
     ]
    }
   ],
   "source": [
    "text_input = \"Describe this image shortly\"\n",
    "image_input = {\"mime_type\": \"image/png\", \"data\": open(\"bella.png\", 'rb').read()}\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    contents=dict(\n",
    "        parts=[dict(text = text_input), dict(inline_data = image_input)]\n",
    "    )\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-tps-de24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
